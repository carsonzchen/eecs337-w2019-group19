{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Import and Package Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "from nltk import bigrams, trigrams\n",
    "import json\n",
    "from pprint import pprint\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load json data\n",
    "dir_path = os.path.dirname(os.path.realpath(\"\"))\n",
    "with open(dir_path + '/eecs337-w2019-group19/data/gg2013.json') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "stopWords = set(stopwords.words('english'))\n",
    "twitterwords = {\"http\", \"rt\", \"goldenglobes\", \"golden\", \"globes\", \"RT\", \"Golden\", \"Globes\", \"GoldenGlobes\"}\n",
    "stopWords = stopWords.union(twitterwords)\n",
    "keyword = ['present', 'presents', 'presented', 'presenting', 'presenter', 'presenters', \n",
    "           'introduce', 'introducing', 'introduces', 'introduce']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combined tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contains(a, b):\n",
    "    a_set = set(a)\n",
    "    b_set = set(b)\n",
    "    if len(a_set.intersection(b_set)) > 0: \n",
    "        return True\n",
    "    else:\n",
    "        return False   \n",
    "\n",
    "def tweetTokenContain(dataset, keywords, stopwords):\n",
    "    tweetnumber = []\n",
    "    tweetlist = []\n",
    "    i = 0\n",
    "\n",
    "    for i in range(len(dataset) - 1):\n",
    "        tweettextToken = []\n",
    "        words = word_tokenize(dataset[i]['text'])\n",
    "        words_clean = [token for token in words if token not in stopwords and token.isalpha()]\n",
    "        \n",
    "        if contains(words_clean, keywords) == True:\n",
    "            tweetnumber.append(i)\n",
    "            tweetlist.append(words_clean)\n",
    "        \n",
    "        i += 1\n",
    "        \n",
    "    return([tweetnumber, tweetlist])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_present_tokenized = tweetTokenContain(data, keyword, stopWords)\n",
    "tct = tweet_present_tokenized[0]\n",
    "tweet_present_tokenized_list = tweet_present_tokenized[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tweetPrint(dataset, tweetnumber):\n",
    "    return [dataset[i]['text'] for i in tweetnumber]\n",
    "\n",
    "tweet_present = tweetPrint(data, tct)\n",
    "len(tweet_present)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bigramsDict(tweetlist):\n",
    "\n",
    "    common_bigrams = {}\n",
    "    \n",
    "    for sentence in tweetlist:\n",
    "        bigrm_in_tweet = list(bigrams(sentence))\n",
    "        for bg in bigrm_in_tweet:\n",
    "            if bg in common_bigrams.keys():\n",
    "                common_bigrams[bg] += 1\n",
    "            else:\n",
    "                common_bigrams[bg] = 1\n",
    "    \n",
    "    d_view = [ (v,k) for k,v in common_bigrams.items() ]\n",
    "    return d_view\n",
    "\n",
    "common_bigrams_dict = bigramsDict(tweet_present_tokenized_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printDictbyCount(dict_view, maximum = 30):\n",
    "    d_view.sort(reverse=True) # natively sort tuples by first element\n",
    "    counter = 0\n",
    "    \n",
    "    for v,k in d_view:\n",
    "        pprint(\"%s: %d\" % (k,v))\n",
    "        counter+=1\n",
    "\n",
    "        if counter > maximum:\n",
    "            break\n",
    "\n",
    "printDictbyCount(common_bigrams_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkFullNameFormat(inputTuple2):\n",
    "    if inputTuple2[0][0].isupper() == True and inputTuple2[1][0].isupper() == True:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractPseudoNames(input_dict, lowbound = 10):\n",
    "\n",
    "    pseudo_namelist = []\n",
    "    \n",
    "    for wordpair in input_dict:\n",
    "        if wordpair[0] >= lowbound and checkFullNameFormat(wordpair[1]) == True:\n",
    "            name_like = wordpair[1][0].lower() + \" \" + wordpair[1][1].lower()\n",
    "            pseudo_namelist.append(name_like)\n",
    "        \n",
    "    return(pseudo_namelist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pseudo_namelist = extractPseudoNames(common_bigrams_dict, lowbound = 10)\n",
    "pseudo_namelist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main method of understanding sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 1: Co-occurrence of names and keywords\n",
    "def cooccur_tweets(names, keywordlist):\n",
    "    tweetnumber = []\n",
    "    i = 0\n",
    "\n",
    "    for i in range(len(data) - 1):\n",
    "        tweetsentence = data[i]['text'].lower()\n",
    "        for name in names:\n",
    "            tweetsplit = tweetsentence.split(name)\n",
    "            if len(tweetsplit) > 1:\n",
    "                for part in tweetsplit:\n",
    "                    if len(part) > 1:\n",
    "                        parttoken = word_tokenize(part)\n",
    "                        if contains(parttoken, keywordlist) == True:\n",
    "                            tweetnumber.append(i)\n",
    "        i += 1\n",
    "    \n",
    "    return(tweetnumber)\n",
    "\n",
    "# Method 2: Appearance of keywords immediately or 1 word after names (Subject + Verb structure)\n",
    "def subjverb_tweets(names, keywordlist):\n",
    "    tweetnumber = []\n",
    "    i = 0\n",
    "\n",
    "    for i in range(len(data) - 1):\n",
    "        tweetsentence = data[i]['text'].lower()\n",
    "        for name in names:\n",
    "            tweetsplit = tweetsentence.split(name)\n",
    "            if len(tweetsplit) > 1:\n",
    "                for part in tweetsplit:\n",
    "                    if len(part) > 1:\n",
    "                        parttoken = word_tokenize(part)\n",
    "                        if len(parttoken) > 1:\n",
    "                            immediateWords = [parttoken[0], parttoken[1]]\n",
    "                        else:\n",
    "                            immediateWords = [parttoken[0]]\n",
    "                        \n",
    "                        if contains(immediateWords, keywordlist) == True:\n",
    "                            tweetnumber.append(i)\n",
    "        i += 1\n",
    "    \n",
    "    return(tweetnumber)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#coc = cooccur_tweets(pseudo_namelist, keyword)\n",
    "subv = subjverb_tweets(pseudo_namelist, keyword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweetPrint(data, subv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test if presenters look right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "presenterData = [data[i] for i in subv]\n",
    "tweet_presenter_tokenized = tweetTokenContain(presenterData, keyword, stopWords)\n",
    "presenter_tokenized_list = tweet_presenter_tokenized[1]\n",
    "pr_common_bigrams = bigramsDict(presenter_tokenized_list)\n",
    "extractPseudoNames(pr_common_bigrams, lowbound = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
